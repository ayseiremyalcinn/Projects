{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lqw82xgTJOoi"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCBV2P7WgCzb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from math import log"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VngeAy7GgNpY",
        "outputId": "287609f6-3f52-4aef-e584-658c2c1465c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read data with Pandas\n",
        "df=pd.read_csv(\"/content/drive/MyDrive/Fall'22/BBM409/deneme/English Dataset.csv\")"
      ],
      "metadata": {
        "id": "6PIbqPRJgRNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "wF9Icdy3hZWU",
        "outputId": "58b0e891-8466-4361-b797-ce4c56f0c382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      ArticleId                                               Text  \\\n",
              "0          1833  worldcom ex-boss launches defence lawyers defe...   \n",
              "1           154  german business confidence slides german busin...   \n",
              "2          1101  bbc poll indicates economic gloom citizens in ...   \n",
              "3          1976  lifestyle  governs mobile choice  faster  bett...   \n",
              "4           917  enron bosses in $168m payout eighteen former e...   \n",
              "...         ...                                                ...   \n",
              "1485        857  double eviction from big brother model caprice...   \n",
              "1486        325  dj double act revamp chart show dj duo jk and ...   \n",
              "1487       1590  weak dollar hits reuters revenues at media gro...   \n",
              "1488       1587  apple ipod family expands market apple has exp...   \n",
              "1489        538  santy worm makes unwelcome visit thousands of ...   \n",
              "\n",
              "           Category  \n",
              "0          business  \n",
              "1          business  \n",
              "2          business  \n",
              "3              tech  \n",
              "4          business  \n",
              "...             ...  \n",
              "1485  entertainment  \n",
              "1486  entertainment  \n",
              "1487       business  \n",
              "1488           tech  \n",
              "1489           tech  \n",
              "\n",
              "[1490 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-799fb251-8a4f-431a-a7a1-1fe66484e0ec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ArticleId</th>\n",
              "      <th>Text</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1833</td>\n",
              "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>154</td>\n",
              "      <td>german business confidence slides german busin...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1101</td>\n",
              "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1976</td>\n",
              "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>917</td>\n",
              "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1485</th>\n",
              "      <td>857</td>\n",
              "      <td>double eviction from big brother model caprice...</td>\n",
              "      <td>entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1486</th>\n",
              "      <td>325</td>\n",
              "      <td>dj double act revamp chart show dj duo jk and ...</td>\n",
              "      <td>entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1487</th>\n",
              "      <td>1590</td>\n",
              "      <td>weak dollar hits reuters revenues at media gro...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1488</th>\n",
              "      <td>1587</td>\n",
              "      <td>apple ipod family expands market apple has exp...</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1489</th>\n",
              "      <td>538</td>\n",
              "      <td>santy worm makes unwelcome visit thousands of ...</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1490 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-799fb251-8a4f-431a-a7a1-1fe66484e0ec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-799fb251-8a4f-431a-a7a1-1fe66484e0ec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-799fb251-8a4f-431a-a7a1-1fe66484e0ec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "df = shuffle(df, random_state = 45)"
      ],
      "metadata": {
        "id": "WjI5y3C0hLLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "fXHv3h23hXF3",
        "outputId": "2496ff4c-91c4-4b65-ca0c-7521ee2c9005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      ArticleId                                               Text  \\\n",
              "374        2168  no re-draft  for eu patent law a proposed euro...   \n",
              "104         684  celebrities get to stay in jungle all four con...   \n",
              "833        2188  jones files conte lawsuit marion jones has fil...   \n",
              "339         739  us duo in first spam conviction a brother and ...   \n",
              "1152       1911  hospital suspends  no welsh  plan an english h...   \n",
              "...         ...                                                ...   \n",
              "580        1926  warning over us pensions deficit taxpayers may...   \n",
              "163        1769  curbishley delight for johansson charlton mana...   \n",
              "607        1498  lib dems unveil election slogan the liberal de...   \n",
              "414        1836  henman overcomes rival rusedski tim henman sav...   \n",
              "971          75  call to save manufacturing jobs the trades uni...   \n",
              "\n",
              "           Category  \n",
              "374            tech  \n",
              "104   entertainment  \n",
              "833           sport  \n",
              "339            tech  \n",
              "1152       politics  \n",
              "...             ...  \n",
              "580        business  \n",
              "163           sport  \n",
              "607        politics  \n",
              "414           sport  \n",
              "971        business  \n",
              "\n",
              "[1490 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9c1099c3-a3b2-42ae-9cfc-a020fac5ad28\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ArticleId</th>\n",
              "      <th>Text</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>374</th>\n",
              "      <td>2168</td>\n",
              "      <td>no re-draft  for eu patent law a proposed euro...</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>684</td>\n",
              "      <td>celebrities get to stay in jungle all four con...</td>\n",
              "      <td>entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>833</th>\n",
              "      <td>2188</td>\n",
              "      <td>jones files conte lawsuit marion jones has fil...</td>\n",
              "      <td>sport</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339</th>\n",
              "      <td>739</td>\n",
              "      <td>us duo in first spam conviction a brother and ...</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1152</th>\n",
              "      <td>1911</td>\n",
              "      <td>hospital suspends  no welsh  plan an english h...</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>580</th>\n",
              "      <td>1926</td>\n",
              "      <td>warning over us pensions deficit taxpayers may...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>1769</td>\n",
              "      <td>curbishley delight for johansson charlton mana...</td>\n",
              "      <td>sport</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>607</th>\n",
              "      <td>1498</td>\n",
              "      <td>lib dems unveil election slogan the liberal de...</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414</th>\n",
              "      <td>1836</td>\n",
              "      <td>henman overcomes rival rusedski tim henman sav...</td>\n",
              "      <td>sport</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>971</th>\n",
              "      <td>75</td>\n",
              "      <td>call to save manufacturing jobs the trades uni...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1490 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c1099c3-a3b2-42ae-9cfc-a020fac5ad28')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9c1099c3-a3b2-42ae-9cfc-a020fac5ad28 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9c1099c3-a3b2-42ae-9cfc-a020fac5ad28');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le=LabelEncoder()\n",
        "df[\"Category\"] = le.fit_transform(df[\"Category\"])\n"
      ],
      "metadata": {
        "id": "fxNNF1EfJ5eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Category\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6F7rrQlRL2GQ",
        "outputId": "a61ac75c-f53e-42a9-b4fd-939b1a14730f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "374     4\n",
              "104     1\n",
              "833     3\n",
              "339     4\n",
              "1152    2\n",
              "       ..\n",
              "580     0\n",
              "163     3\n",
              "607     2\n",
              "414     3\n",
              "971     0\n",
              "Name: Category, Length: 1490, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[\"Text\"]\n",
        "y = df[\"Category\"]"
      ],
      "metadata": {
        "id": "_kTJP1UDydcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveBayes:\n",
        "    def __init__(self):\n",
        "      self.prior_prob = []\n",
        "      self.num_text = []\n",
        "      self.Unigram_Bow = []\n",
        "      self.Bigram_Bow = []\n",
        "      self.categories = []\n",
        "\n",
        "    def train(self, X_train, y_train, stop_words=[]):\n",
        "      self.X_train = X_train\n",
        "      self.y_train = y_train\n",
        "      self.train=pd.concat([X_train, y_train], axis=1)\n",
        "      self.Unigram_Bow = self.calculate_Bow(self.train,1,stop_words)\n",
        "      self.Bigram_Bow = self.calculate_Bow(self.train,2,stop_words)\n",
        "      \n",
        "      self.prior_prob = [0 for i in range(len(self.num_text))]                  # calculate prior probabilities of categories \n",
        "      for i in range(len(self.num_text)): \n",
        "        self.prior_prob[i] = log(self.num_text[i]/sum(self.num_text))           # category count/ all categories\n",
        "\n",
        "    def train_TF_IDF(self, X_train, y_train,stop_words=[]):\n",
        "      self.X_train = X_train\n",
        "      self.y_train = y_train\n",
        "      self.train=pd.concat([X_train, y_train], axis=1)\n",
        "      self.Unigram_Bow = self.tf_idf(self.train,1,stop_words)\n",
        "      self.Bigram_Bow = self.tf_idf(self.train,2,stop_words)\n",
        "      \n",
        "      self.prior_prob = [0 for i in range(len(self.num_text))]                  # calculate prior probabilities of categories \n",
        "      for i in range(len(self.num_text)):\n",
        "        self.prior_prob[i] = log(self.num_text[i]/sum(self.num_text))           # category count/ all categories\n",
        "\n",
        "    \n",
        "    \n",
        "    def predict(self, X_test, n_gram):\n",
        "        y_pred=[]\n",
        "        for x in X_test:\n",
        "            y_pred.append(self.make_predict(x, n_gram))\n",
        "        return y_pred\n",
        "\n",
        "    # Creates BoW using CountVectorizer\n",
        "    def calculate_Bow(self, train, n_gram,stop_words):\n",
        "      self.categories = train[\"Category\"].unique()\n",
        "      self.num_text = [0 for i in range(len(self.categories))]\n",
        "      bow = [\"\" for i in range(len(self.categories))]                           \n",
        "\n",
        "      for category in range(len(self.categories)):                              # BoW will be an array of all strings in the category when the loop is over\n",
        "        df_bow=train[(train[\"Category\"]==category)] \n",
        "        bow[category] = df_bow[\"Text\"]\n",
        "        self.num_text[category] += len(df_bow)\n",
        "\n",
        "      for i in range(len(bow)):\n",
        "        vectorizer = CountVectorizer(lowercase = True, ngram_range= (n_gram,n_gram),stop_words=stop_words)    # create Vectorizer object\n",
        "        count_matrix = vectorizer.fit_transform(bow[i])\n",
        "        count_array = count_matrix.toarray()        \n",
        "        bow_df = pd.DataFrame(data=count_array,columns = vectorizer.get_feature_names_out())  \n",
        "        bow[i] = bow_df.sum(axis=0).to_dict()                                   # create dictionary with words\n",
        "        \n",
        "      self.unique_texts = {}\n",
        "\n",
        "      for d in [bow[0], bow[1], bow[2], bow[3], bow[4]]:\n",
        "        self.unique_texts.update(d)                       # we will use at log calculations\n",
        "\n",
        "      return bow\n",
        "    \n",
        "    def tf_idf(self, train, n_gram,stop_words):\n",
        "      self.categories = train[\"Category\"].unique()\n",
        "      self.num_text = [0 for i in range(len(self.categories))]\n",
        "      bow = [\"\" for i in range(len(self.categories))]\n",
        "      for category in range(len(self.categories)):\n",
        "        df_bow=train[(train[\"Category\"]==category)]\n",
        "        bow[category] = df_bow[\"Text\"]\n",
        "        self.num_text[category] += len(df_bow)\n",
        "\n",
        "      for i in range(len(bow)):\n",
        "        # Create a Vectorizer Object\n",
        "        vectorizer = TfidfVectorizer(lowercase=True,ngram_range=(n_gram,n_gram),stop_words=stop_words) # create Vectorizer object\n",
        "        count_matrix = vectorizer.fit_transform(bow[i])\n",
        "        count_array = count_matrix.toarray()\n",
        "        bow_df = pd.DataFrame(data=count_array,columns = vectorizer.get_feature_names_out())\n",
        "        bow[i] = bow_df.sum(axis=0).to_dict()          # create dictionary with words\n",
        "\n",
        "        \n",
        "\n",
        "      self.unique_texts = {}\n",
        "      for d in [bow[0], bow[1], bow[2], bow[3], bow[4]]:    # we will use at log calculations\n",
        "        self.unique_texts.update(d)\n",
        "      return bow\n",
        "\n",
        "\n",
        "    def make_predict(self, x, n_gram):\n",
        "      category_score = [1 for i in range(len(self.categories))]\n",
        "      if n_gram == 2:\n",
        "        words = x.split()\n",
        "        words = list(map(' '.join, zip(words[:-1], words[1:])))\n",
        "        BoW = self.Bigram_Bow\n",
        "      else:\n",
        "        words = x.split(\" \")\n",
        "        BoW = self.Unigram_Bow\n",
        "      for word in words:\n",
        "        for i in range(len(category_score)):\n",
        "          if word in BoW[i].keys():\n",
        "            category_score[i] += log((BoW[i][word] + 1) /( sum(BoW[i].values()) + len(self.unique_texts.keys())))\n",
        "          else:\n",
        "            category_score[i] += log(1 / (sum(BoW[i].values()) + len(self.unique_texts.keys())))\n",
        "\n",
        "      for i in range(len(category_score)):\n",
        "        category_score[i] += self.prior_prob[i]\n",
        "      return np.argmax(category_score)\n",
        "\n",
        "    def get_most_presence_n_words(self,ngram,nWord):\n",
        "      for i in range(len(self.Unigram_Bow)):\n",
        "        print(le.inverse_transform([i])[0],\":\")\n",
        "        if(ngram==1):\n",
        "          print(sorted(self.Unigram_Bow[i].items(), key=lambda item: item[1] , reverse=True)[:nWord])\n",
        "        else:\n",
        "          print(sorted(self.Bigram_Bow[i].items(), key=lambda item: item[1] , reverse=True)[:nWord])\n",
        "\n",
        "    def get_most_absence_n_words(self,ngram,nWord):\n",
        "      for i in range(len(self.Unigram_Bow)):\n",
        "        print(le.inverse_transform([i])[0],\":\")\n",
        "        if(ngram==1):\n",
        "          print(sorted(self.Unigram_Bow[i].items(), key=lambda item: item[1])[:nWord])\n",
        "        else:\n",
        "          print(sorted(self.Bigram_Bow[i].items(), key=lambda item: item[1])[:nWord])\n",
        "\n",
        "    def get_list_of_words(self,words):\n",
        "      for i in range(len(self.Unigram_Bow)):\n",
        "        print(le.inverse_transform([i])[0],\":\")\n",
        "        for key in words:\n",
        "          print(key,\"->\",self.Unigram_Bow[i][key],\"   \",end=\"\")\n",
        "        print()\n",
        "      "
      ],
      "metadata": {
        "id": "-WUmtp746Ats"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42) "
      ],
      "metadata": {
        "id": "SBpo86ynK804"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Understanding Data**"
      ],
      "metadata": {
        "id": "588u0B9IMmfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Here is some word with their occurence in all data can show as their categories.* \n",
        "\n",
        "We can see the occurrences of several words `\"mr\", \"new\", \"year\"`. If we want to make a comment, `\"mr\"` appears more in politics, that is, the word `\"mr\"` in political texts affects the classification. Since `\"mr\"` is used the least in sports, the word `\"mr\"` has little effect in sports text. For `\"mr\"` we can say that it is a word that has an effect on classification. However, the occurrences of the words `\"new\"` and `\"year\"` are almost the same for each category. We think that these words do not have a huge impact on categorization."
      ],
      "metadata": {
        "id": "qmmtMfjEeEMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = NaiveBayes()\n",
        "clf.train(X, y)\n",
        "clf.get_list_of_words([\"mr\",\"new\",\"year\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q0LEHSINy0W",
        "outputId": "5c169400-7fbe-41f9-e6d2-4292e69e29a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "business :\n",
            "mr -> 393    new -> 273    year -> 456    \n",
            "entertainment :\n",
            "mr -> 151    new -> 234    year -> 315    \n",
            "politics :\n",
            "mr -> 1073    new -> 280    year -> 175    \n",
            "sport :\n",
            "mr -> 8    new -> 202    year -> 331    \n",
            "tech :\n",
            "mr -> 349    new -> 349    year -> 251    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A) **Analyzing effect of the words on prediction**"
      ],
      "metadata": {
        "id": "oUW1IpahJYT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**List the 10 words whose presence most strongly predicts that the article belongs to specific category for each five categories.**"
      ],
      "metadata": {
        "id": "LYxLqgJsIu8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In below we see 10 words whose presence most strongly predicts in Unigram for all categories.\n",
        "\n",
        "All of them have some common words in presence most strongly predicts 10 words. These are;\n",
        "`\"the\",\"to\",\"of\",\"in\",\"and\",\"for\"`. However all of these are stopwords. We will estimate without stopwords next part."
      ],
      "metadata": {
        "id": "9Gj07NaK76YJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = NaiveBayes()\n",
        "clf.train_TF_IDF(X_train, y_train)\n",
        "clf.get_most_presence_n_words(1,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnRMrFyOKGjS",
        "outputId": "3628cb84-e650-468b-ae08-441b837cdf8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "business :\n",
            "[('the', 70.95764876462664), ('to', 32.817908709006126), ('of', 28.381825584034942), ('in', 28.341438544205033), ('and', 21.49897057026608), ('said', 12.296416780507798), ('it', 11.707978624765063), ('that', 11.550008739780894), ('is', 11.539364304631231), ('for', 11.346785316842237)]\n",
            "entertainment :\n",
            "[('the', 57.419369042051514), ('and', 20.560072226707508), ('to', 20.442022091704242), ('in', 20.237870447845136), ('of', 19.55640512893384), ('for', 11.883231662140942), ('film', 10.201944728398576), ('was', 9.612219243218215), ('on', 9.368725763545408), ('he', 8.588226115086652)]\n",
            "politics :\n",
            "[('the', 75.04298296233469), ('to', 36.480312482345994), ('of', 26.479894877984233), ('and', 24.020639227563237), ('in', 20.167782173741294), ('he', 15.765589064105303), ('said', 15.05334039825951), ('mr', 13.216505626266827), ('for', 12.068284324934092), ('on', 12.0083662771412)]\n",
            "sport :\n",
            "[('the', 67.17049428380793), ('to', 33.000835379545556), ('in', 27.01756569922483), ('and', 26.034580024687166), ('of', 18.938557586897364), ('he', 14.90917282345732), ('we', 13.358168714284155), ('is', 13.218213565381758), ('for', 13.15635147179857), ('it', 12.846580375424798)]\n",
            "tech :\n",
            "[('the', 60.37144975035937), ('to', 33.19626408033648), ('of', 28.07767878667242), ('and', 23.772530159374234), ('in', 19.475598899113905), ('that', 13.332584628949201), ('is', 13.200636842786766), ('it', 11.69686345400729), ('for', 10.752085206431063), ('on', 9.826949402424113)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In below we see 10 words whose presence most strongly predicts in Bigram for all categories.\n",
        "\n",
        "All of them have some common words in absence most strongly predicts 10 words. These are;\n",
        "`\"of the\",\"in the\"`. However all of these are stopwords. We will estimate without stopwords next part. \n",
        "\n",
        "At **business** we can see `\"the us\",\"the company\" `, at **entertainment** we can see `\"the film\"`, at politics we can see `\"mr blair\", \"mr brown\", \"the government\", \"mr brown` in the most occurence 10 words. We know that this words related with their categories."
      ],
      "metadata": {
        "id": "VsxUdWAXK__u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf.get_most_presence_n_words(2,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qz5SyNHWKHJm",
        "outputId": "4aa3703c-e20e-4b38-b10f-d232a95be4d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "business :\n",
            "[('of the', 7.053731445048548), ('in the', 6.9666798859466175), ('the us', 4.333071282212917), ('for the', 3.9179350102273984), ('to the', 3.8436881223119026), ('on the', 3.7008120290487305), ('that the', 3.435768602930095), ('the company', 3.1611125893129244), ('said the', 3.0991054592390554), ('said it', 2.9581942291642505)]\n",
            "entertainment :\n",
            "[('of the', 6.011593587306114), ('in the', 5.814181828536041), ('at the', 3.724031523881116), ('for the', 3.2146830666348114), ('to the', 2.9649334339480546), ('will be', 2.9263098955654336), ('to be', 2.8682747685786842), ('on the', 2.866522527060242), ('the film', 2.8505923458559153), ('and the', 2.7566326619692525)]\n",
            "politics :\n",
            "[('of the', 6.468304102484044), ('in the', 5.392737166423687), ('to the', 4.472174297497518), ('mr blair', 4.1535517520662175), ('he said', 4.002772737377743), ('mr brown', 3.779606115821695), ('said the', 3.7641143338975933), ('the government', 3.6712174504812483), ('to be', 3.6304921911696693), ('on the', 3.4041159375777186)]\n",
            "sport :\n",
            "[('in the', 8.804956578230325), ('of the', 5.904174440531972), ('at the', 4.499837548293993), ('for the', 4.334293249607523), ('on the', 3.9427873418709605), ('to the', 3.6419796358764778), ('the first', 3.2182554115150994), ('to be', 3.0265388997276728), ('it was', 3.0128961231918527), ('he said', 2.953741267518103)]\n",
            "tech :\n",
            "[('of the', 6.346698555155582), ('in the', 5.101079094741), ('will be', 3.358684025776953), ('he said', 3.3208540280744203), ('on the', 3.1240095647214168), ('to be', 3.0778305749019608), ('to the', 3.0061412109550303), ('it is', 2.7734178749167846), ('for the', 2.7428231239308625), ('more than', 2.521632339974962)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**List the 10 words whose absence most strongly predicts that the article belongs to specific category for each five categories.**"
      ],
      "metadata": {
        "id": "8rVVRei5JCwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that this words so unrelated with their categories. In **sport** we clearly see these words are absence because these words corresponding year. Presumably, when we try to predict a text, it is very unlikely that the new text will include these years."
      ],
      "metadata": {
        "id": "3Z6JVNEVW7q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = NaiveBayes()\n",
        "clf.train_TF_IDF(X_train, y_train)\n",
        "clf.get_most_absence_n_words(1,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfzFrymEIua0",
        "outputId": "afa57c9a-b66e-4c68-90c9-4ddfca99c519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "business :\n",
            "[('accessing', 0.03171291265321466), ('adequate', 0.03171291265321466), ('boundary', 0.03171291265321466), ('bracing', 0.03171291265321466), ('brighten', 0.03171291265321466), ('calculate', 0.03171291265321466), ('carefully', 0.03171291265321466), ('chairs', 0.03171291265321466), ('checked', 0.03171291265321466), ('circuits', 0.03171291265321466)]\n",
            "entertainment :\n",
            "[('betting', 0.023885660921501378), ('certainty', 0.023885660921501378), ('comprised', 0.023885660921501378), ('darabont', 0.023885660921501378), ('flip', 0.023885660921501378), ('kane', 0.023885660921501378), ('outsiders', 0.023885660921501378), ('overwhelmingly', 0.023885660921501378), ('paper', 0.023885660921501378), ('points', 0.023885660921501378)]\n",
            "politics :\n",
            "[('20p', 0.010963905404179271), ('3rds', 0.010963905404179271), ('3x', 0.010963905404179271), ('5000', 0.010963905404179271), ('50pc', 0.010963905404179271), ('75p', 0.010963905404179271), ('80s', 0.010963905404179271), ('absorb', 0.010963905404179271), ('adair', 0.010963905404179271), ('afloat', 0.010963905404179271)]\n",
            "sport :\n",
            "[('1870', 0.018927344536602057), ('1888', 0.018927344536602057), ('1893', 0.018927344536602057), ('1903', 0.018927344536602057), ('1908', 0.018927344536602057), ('1921', 0.018927344536602057), ('1924', 0.018927344536602057), ('1928', 0.018927344536602057), ('1930', 0.018927344536602057), ('1935', 0.018927344536602057)]\n",
            "tech :\n",
            "[('amazed', 0.02334849239308104), ('astro', 0.02334849239308104), ('brits', 0.02334849239308104), ('cant', 0.02334849239308104), ('coffee', 0.02334849239308104), ('compass', 0.02334849239308104), ('diamond', 0.02334849239308104), ('divides', 0.02334849239308104), ('fizzy', 0.02334849239308104), ('froth', 0.02334849239308104)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "MG9ZFsAXHrCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that this pairs are so unrelated. Like `\"000 years\" `, `\"12 more\" `, `\"and 68\"`."
      ],
      "metadata": {
        "id": "UQ66ULwvYVnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf.get_most_absence_n_words(2,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILg-gDS5PS5f",
        "outputId": "d0320579-933e-4ab1-ae5b-5ace17c1506a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "business :\n",
            "[('about your', 0.03449869478745337), ('accessing the', 0.03449869478745337), ('additional bracing', 0.03449869478745337), ('additional wireless', 0.03449869478745337), ('adequate ideally', 0.03449869478745337), ('advance as', 0.03449869478745337), ('advise on', 0.03449869478745337), ('allow good', 0.03449869478745337), ('allow you', 0.03449869478745337), ('allows you', 0.03449869478745337)]\n",
            "entertainment :\n",
            "[('1994 frank', 0.022496920764156634), ('2005 presenter', 0.022496920764156634), ('52 of', 0.022496920764156634), ('68 of', 0.022496920764156634), ('actress alongside', 0.022496920764156634), ('all expect', 0.022496920764156634), ('alongside kate', 0.022496920764156634), ('already seen', 0.022496920764156634), ('also revealed', 0.022496920764156634), ('and 68', 0.022496920764156634)]\n",
            "politics :\n",
            "[('00 mark', 0.01347228697352125), ('000 our', 0.01347228697352125), ('05 gives', 0.01347228697352125), ('05 is', 0.01347228697352125), ('05 the', 0.01347228697352125), ('10 for', 0.01347228697352125), ('11 000', 0.01347228697352125), ('12 000', 0.01347228697352125), ('12 more', 0.01347228697352125), ('16 21', 0.01347228697352125)]\n",
            "sport :\n",
            "[('10 with', 0.021411477726906717), ('107 matches', 0.021411477726906717), ('11 in', 0.021411477726906717), ('12 10', 0.021411477726906717), ('12 although', 0.021411477726906717), ('12 at', 0.021411477726906717), ('13 in', 0.021411477726906717), ('14 may', 0.021411477726906717), ('16 by', 0.021411477726906717), ('1870 the', 0.021411477726906717)]\n",
            "tech :\n",
            "[('000 years', 0.02645742422372497), ('100 gadgets', 0.02645742422372497), ('1990s toy', 0.02645742422372497), ('84th almost', 0.02645742422372497), ('98th 1990s', 0.02645742422372497), ('about astro', 0.02645742422372497), ('almost everyone', 0.02645742422372497), ('am amazed', 0.02645742422372497), ('amazed by', 0.02645742422372497), ('american authors', 0.02645742422372497)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B) **Stop Words**"
      ],
      "metadata": {
        "id": "lqw82xgTJOoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "english_stopwords=list(ENGLISH_STOP_WORDS)\n",
        "english_stopwords.append('said')\n",
        "print(english_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf39ZuX3HS_J",
        "outputId": "74db5aab-27af-4300-8069-bc17195a8ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['show', 'becomes', 'four', 'noone', 'whereby', 'hereby', 'indeed', 'thence', 'still', 'own', 'anyhow', 're', 'through', 'before', 'there', 'due', 'whither', 'eight', 'hundred', 'fill', 'this', 'has', 'first', 'also', 'give', 'none', 'eg', 'empty', 'least', 'name', 'must', 'itself', 'hereafter', 'sometimes', 'cannot', 'or', 'few', 'please', 'moreover', 'further', 'we', 'eleven', 'either', 'found', 'via', 'un', 'and', 'the', 'not', 'made', 'beyond', 'those', 'when', 'rather', 'can', 'themselves', 'a', 'many', 'toward', 'etc', 'may', 'against', 'whether', 'becoming', 'both', 'each', 'elsewhere', 'other', 'hereupon', 'ten', 'are', 'became', 'whatever', 'by', 'throughout', 'whereas', 'amoungst', 'ever', 'whole', 'during', 'all', 'latterly', 'would', 'yours', 'ltd', 'how', 'three', 'together', 'no', 'more', 'whence', 'co', 'such', 'which', 'even', 'most', 'con', 'whose', 'every', 'meanwhile', 'its', 'others', 'above', 'mill', 'what', 'de', 'less', 'much', 'ie', 'system', 'for', 'same', 'thick', 'thus', 'until', 'therein', 'enough', 'that', 'bottom', 'over', 'well', 'hers', 'along', 'upon', 'already', 'an', 'they', 'nobody', 'hasnt', 'another', 'interest', 'beside', 'latter', 'were', 'amount', 'keep', 'she', 'seeming', 'put', 'hence', 'next', 'thru', 'but', 'serious', 'wherein', 'seems', 'everywhere', 'so', 'amongst', 'without', 'always', 'per', 'bill', 'as', 'call', 'my', 'very', 'off', 'any', 'seem', 'he', 'see', 'below', 'back', 'at', 'nevertheless', 'side', 'detail', 'describe', 'on', 'was', 'inc', 'someone', 'else', 'is', 'twelve', 'thereafter', 'once', 'anyone', 'fifteen', 'if', 'around', 'to', 'third', 'among', 'onto', 'somehow', 'only', 'anywhere', 'her', 'yourself', 'besides', 'fifty', 'while', 'though', 'fire', 'alone', 'behind', 'couldnt', 'mine', 'take', 'full', 'their', 'find', 'nor', 'often', 'us', 'been', 'himself', 'thin', 'why', 'with', 'myself', 'too', 'done', 'although', 'nowhere', 'six', 'you', 'thereupon', 'in', 'from', 'one', 'should', 'his', 'part', 'these', 'now', 'could', 'however', 'nine', 'almost', 'across', 'neither', 'twenty', 'down', 'them', 'top', 'between', 'because', 'thereby', 'go', 'since', 'namely', 'sincere', 'nothing', 'never', 'had', 'here', 'into', 'ourselves', 'whom', 'everyone', 'being', 'wherever', 'under', 'front', 'after', 'whereupon', 'two', 'again', 'move', 'him', 'up', 'otherwise', 'get', 'cry', 'last', 'somewhere', 'perhaps', 'will', 'beforehand', 'herein', 'i', 'of', 'whereafter', 'might', 'towards', 'yet', 'it', 'be', 'do', 'seemed', 'have', 'me', 'except', 'sometime', 'who', 'ours', 'out', 'something', 'several', 'formerly', 'our', 'then', 'cant', 'afterwards', 'anything', 'herself', 'than', 'therefore', 'everything', 'five', 'yourselves', 'your', 'whenever', 'sixty', 'whoever', 'anyway', 'become', 'within', 'former', 'some', 'forty', 'mostly', 'where', 'about', 'am', 'said']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**List the 10 words whose presence most strongly predicts that the article belongs to specific category for each five categories.** \n",
        "\n",
        "**Without stop words!!!**"
      ],
      "metadata": {
        "id": "rOWYNuakFzSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In below we see the most presence 10 words without stop words in Unigram for all categories.\n",
        "\n",
        "We see business, politics and tech categories includes `\"mr\"` and all of them includes `\"new\"`. These words like stop words' behaviour. The other words related with their categories."
      ],
      "metadata": {
        "id": "AepaCYdvcxJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = NaiveBayes()\n",
        "clf.train_TF_IDF(X_train, y_train,english_stopwords)\n",
        "clf.get_most_presence_n_words(1,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Dwaj0M2Dj2o",
        "outputId": "22f0050f-b01c-4cf6-db6e-437bf81d089a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "business :\n",
            "[('year', 7.885523855358925), ('mr', 7.524265899440198), ('sales', 6.604490735213493), ('growth', 6.482519702778017), ('economy', 6.30511773311268), ('oil', 5.857026406753185), ('bank', 5.82927728165229), ('market', 5.823054761944759), ('firm', 5.719463036419863), ('new', 5.558714949453762)]\n",
            "entertainment :\n",
            "[('film', 11.516880650933699), ('best', 9.209039605076942), ('year', 5.825435631126013), ('music', 5.388407158514384), ('band', 5.355127232993832), ('number', 5.276699275502582), ('awards', 4.81662088258036), ('award', 4.571490416508048), ('new', 4.460998858284408), ('actor', 4.279137954590733)]\n",
            "politics :\n",
            "[('mr', 15.610656703727146), ('labour', 10.314072500816552), ('election', 9.445650924996293), ('blair', 9.366086438709702), ('brown', 8.137938428556842), ('party', 8.012039320929732), ('government', 7.3125045282339185), ('people', 6.2103314542569334), ('tax', 6.136255378599537), ('howard', 5.798747874037895)]\n",
            "sport :\n",
            "[('england', 9.310548900628532), ('game', 7.499451211653275), ('year', 6.922946256246446), ('win', 6.436823675591233), ('world', 6.368941697373067), ('time', 6.040578339944268), ('ireland', 6.023051182010688), ('wales', 5.796620071717943), ('play', 5.496587787538412), ('team', 5.4788944444382315)]\n",
            "tech :\n",
            "[('people', 8.019698843096108), ('mobile', 7.337636102614999), ('mr', 5.656373511299757), ('phone', 5.540468425566393), ('software', 5.399295329027769), ('music', 5.384832654140668), ('net', 5.161113724304576), ('new', 5.00385470733908), ('microsoft', 4.99209175193782), ('users', 4.891658827259019)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In below we see the most presence 10 words without stop words in Bigram for all categories.\n",
        "\n",
        "In here the words so correlated with their categories. (We think the special name `mr brown` or `mr ebbers` may be important name in their categories.)"
      ],
      "metadata": {
        "id": "2aH8DxetgEk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf.get_most_presence_n_words(2,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlTPu4znPZjl",
        "outputId": "56a6db03-a267-4dcc-f8a5-e3118ce89493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "business :\n",
            "[('chief executive', 2.239593040310824), ('economic growth', 1.8588226628662257), ('mr ebbers', 1.6257621463849823), ('deutsche boerse', 1.5743386640953765), ('oil prices', 1.5742343878153524), ('new york', 1.4914961773957542), ('mr glazer', 1.4502461528286679), ('stock market', 1.3808746765179782), ('fourth quarter', 1.2742919214848885), ('sri lanka', 1.1686755446278534)]\n",
            "entertainment :\n",
            "[('box office', 2.448913141675062), ('new york', 1.7008386102244137), ('year old', 1.5921039437910196), ('won best', 1.509565482699943), ('named best', 1.5004966885625757), ('vera drake', 1.4501183673485736), ('los angeles', 1.4159331142791591), ('million dollar', 1.400896114129235), ('dollar baby', 1.362335039819239), ('best actress', 1.3551563790385242)]\n",
            "politics :\n",
            "[('mr blair', 5.26336884540814), ('mr brown', 4.755965642136332), ('prime minister', 4.138005799647052), ('mr howard', 2.970734057762314), ('general election', 2.8292065477700152), ('tony blair', 2.734082412586757), ('kilroy silk', 2.447744527801727), ('mr kennedy', 2.3905495035988005), ('lib dems', 2.3452941443116297), ('told bbc', 2.224384596150387)]\n",
            "sport :\n",
            "[('year old', 3.3576372283337754), ('australian open', 2.3972019879530904), ('champions league', 2.2722341821396617), ('davis cup', 2.1096511576577646), ('grand slam', 1.9990880734644003), ('new zealand', 1.9663962611646857), ('told bbc', 1.9093264891972095), ('world cup', 1.767237872325131), ('cross country', 1.7552095307237312), ('manchester united', 1.752353166917698)]\n",
            "tech :\n",
            "[('mobile phone', 2.0395063348188263), ('mobile phones', 1.667796758566017), ('anti virus', 1.6458151290195544), ('high definition', 1.5214954580588707), ('bbc news', 1.4492817831460634), ('told bbc', 1.426900868062131), ('news website', 1.2428219858234573), ('ask jeeves', 1.226060274219627), ('consumer electronics', 1.1721622687836073), ('wi fi', 1.1056304690598002)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "UCkK9LztKx5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**List the 10 words whose absence most strongly predicts that the article belongs to specific category for each five categories.**\n",
        "\n",
        "**Without stop words!!!**"
      ],
      "metadata": {
        "id": "4K62xan1FjMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that this words so unrelated with their categories. We can give an example same as with stop words part. \n",
        "\n",
        "If we want to give another unrelated some words , these are the numbers such as `\"118\"`, `\"925p\"`, `\"50pc\"`. "
      ],
      "metadata": {
        "id": "17Y4gl34703n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = NaiveBayes()\n",
        "clf.train_TF_IDF(X_train, y_train,english_stopwords)\n",
        "clf.get_most_absence_n_words(1,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VvLRy5BPw4U",
        "outputId": "8e506973-7b61-409a-d59c-53f626de5bb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "business :\n",
            "[('118', 0.039385010089065996), ('44bn', 0.039385010089065996), ('925p', 0.039385010089065996), ('accretive', 0.039385010089065996), ('branch', 0.039385010089065996), ('caught', 0.039385010089065996), ('comfortable', 0.039385010089065996), ('conservatively', 0.039385010089065996), ('crippled', 0.039385010089065996), ('davies', 0.039385010089065996)]\n",
            "entertainment :\n",
            "[('betting', 0.025140694296576355), ('certainty', 0.025140694296576355), ('comprised', 0.025140694296576355), ('darabont', 0.025140694296576355), ('flip', 0.025140694296576355), ('kane', 0.025140694296576355), ('outsiders', 0.025140694296576355), ('overwhelmingly', 0.025140694296576355), ('paper', 0.025140694296576355), ('points', 0.025140694296576355)]\n",
            "politics :\n",
            "[('20p', 0.013798751654267033), ('3rds', 0.013798751654267033), ('3x', 0.013798751654267033), ('5000', 0.013798751654267033), ('50pc', 0.013798751654267033), ('75p', 0.013798751654267033), ('80s', 0.013798751654267033), ('absorb', 0.013798751654267033), ('adair', 0.013798751654267033), ('afloat', 0.013798751654267033)]\n",
            "sport :\n",
            "[('1870', 0.024159700068338032), ('1888', 0.024159700068338032), ('1893', 0.024159700068338032), ('1903', 0.024159700068338032), ('1908', 0.024159700068338032), ('1921', 0.024159700068338032), ('1924', 0.024159700068338032), ('1928', 0.024159700068338032), ('1930', 0.024159700068338032), ('1935', 0.024159700068338032)]\n",
            "tech :\n",
            "[('abroad', 0.03071866887545636), ('bragging', 0.03071866887545636), ('chilean', 0.03071866887545636), ('commercialised', 0.03071866887545636), ('contra', 0.03071866887545636), ('cuban', 0.03071866887545636), ('diego', 0.03071866887545636), ('djs', 0.03071866887545636), ('economically', 0.03071866887545636), ('eminem', 0.03071866887545636)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non-stop words absence most of pairs included numbers in all categories."
      ],
      "metadata": {
        "id": "1vz3G9nvI9Qg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf.get_most_absence_n_words(2,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeaTZs7UPxAy",
        "outputId": "05e101ff-82c9-43c3-b3ad-d34a57d05e5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "business :\n",
            "[('100 decline', 0.040262963262219044), ('100 kfb', 0.040262963262219044), ('118 million', 0.040262963262219044), ('16 future', 0.040262963262219044), ('1997 asia', 0.040262963262219044), ('1999 kfb', 0.040262963262219044), ('2006 standard', 0.040262963262219044), ('22 group', 0.040262963262219044), ('28 pence', 0.040262963262219044), ('40 economy', 0.040262963262219044)]\n",
            "entertainment :\n",
            "[('1994 frank', 0.024006727991581312), ('2005 presenter', 0.024006727991581312), ('52 online', 0.024006727991581312), ('68 text', 0.024006727991581312), ('ability flip', 0.024006727991581312), ('according uk', 0.024006727991581312), ('actor actress', 0.024006727991581312), ('actress alongside', 0.024006727991581312), ('actress ray', 0.024006727991581312), ('alongside kate', 0.024006727991581312)]\n",
            "politics :\n",
            "[('00 mark', 0.017935720871240974), ('000 losses', 0.017935720871240974), ('000 minimum', 0.017935720871240974), ('000 raising', 0.017935720871240974), ('000 unions', 0.017935720871240974), ('05 gives', 0.017935720871240974), ('05 hardly', 0.017935720871240974), ('05 minimum', 0.017935720871240974), ('10 aged', 0.017935720871240974), ('11 000', 0.017935720871240974)]\n",
            "sport :\n",
            "[('10 minute', 0.027301690016491555), ('107 matches', 0.027301690016491555), ('11 cardiff', 0.027301690016491555), ('12 arms', 0.027301690016491555), ('12 winning', 0.027301690016491555), ('13 cardiff', 0.027301690016491555), ('14 1870', 0.027301690016491555), ('15 dublin', 0.027301690016491555), ('16 blacks', 0.027301690016491555), ('1870 town', 0.027301690016491555)]\n",
            "tech :\n",
            "[('000 years', 0.035972149347855836), ('100 gadgets', 0.035972149347855836), ('1990s toy', 0.035972149347855836), ('84th mobile', 0.035972149347855836), ('98th 1990s', 0.035972149347855836), ('amazed obsession', 0.035972149347855836), ('american authors', 0.035972149347855836), ('americans focus', 0.035972149347855836), ('apple tv', 0.035972149347855836), ('astro wars', 0.035972149347855836)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analyzing effect of the stopwords: Why might it make sense to remove stop words when interpreting the model? Why might it make sense to keep stop words?**\n",
        "\n",
        "In unigram remove stop-words is so effective. Because most of text include stop-words for predict we mostly need unique words. \n",
        "\n",
        "However in bigram it can effect bad. Because if we want to give text with stop-word in bigram, the model can't recognize some pairs like \"won best\" because this pair give in model like -> \"won the\" and \"the best\"."
      ],
      "metadata": {
        "id": "UDRShERXcOkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Accuracy**"
      ],
      "metadata": {
        "id": "WnOfaDLhJ_Wd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*With stop words*"
      ],
      "metadata": {
        "id": "l30EEf-GMC63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = NaiveBayes()\n",
        "clf.train(X_train, y_train)\n",
        "y_pred = clf.predict(X_test, 1)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Unigram accuracy:\",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTcNUW23xYig",
        "outputId": "d6ce4432-b683-4393-a662-1761e977fabc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram accuracy: 0.9194630872483222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(X_test, 2)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Bigram accuracy:\",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gOFHlvzKcQ8",
        "outputId": "d4115d26-3468-4aa0-dce6-111758a30dc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram accuracy: 0.9463087248322147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*With stop words TF-IDF*"
      ],
      "metadata": {
        "id": "ekyFDROIMJ05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = NaiveBayes()\n",
        "clf.train_TF_IDF(X_train, y_train)\n",
        "y_pred = clf.predict(X_test, 1)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"TF-IDF unigram accuracy\",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN6aqkNMiLfA",
        "outputId": "894e96ee-6b94-42ee-be41-a918ac8494fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF unigram accuracy 0.8322147651006712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(X_test, 2)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"TF-IDF bigram accuracy\",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtXhK-oCLomi",
        "outputId": "ac078fbf-cfb3-4bd0-84ee-b259311de21b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF bigram accuracy 0.8993288590604027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*No stop words*"
      ],
      "metadata": {
        "id": "vkwjNzwtMRj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = NaiveBayes()\n",
        "clf.train(X_train, y_train,english_stopwords)\n",
        "y_pred = clf.predict(X_test, 1)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Non-stopword unigram accuracy:\",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6aeQlyNK4Uu",
        "outputId": "c6d61d99-c242-4382-834f-612c9f57b613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-stopword unigram accuracy: 0.959731543624161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(X_test, 2)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Non-stopword bigram accuracy:\",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkJlHr1qLgrT",
        "outputId": "92d3de36-a5b6-467b-b83f-dfdf4e7dce3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-stopword bigram accuracy: 0.7516778523489933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*No stop words TF-IDF*"
      ],
      "metadata": {
        "id": "OPcmhW7wMXmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = NaiveBayes()\n",
        "clf.train_TF_IDF(X_train, y_train,english_stopwords)\n",
        "y_pred = clf.predict(X_test, 1)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"TF-IDF unigram accuracy\",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kD8l_CJLyJe",
        "outputId": "5177c9e4-4d9b-4932-eb51-6bd80efedde5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF unigram accuracy 0.9463087248322147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(X_test, 2)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"TF-IDF bigram accuracy\",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0YcgmU6LyMa",
        "outputId": "aac9ff4e-18e0-4def-bc54-9da33013ad88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF bigram accuracy 0.9530201342281879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**With Stop Words**\n",
        "\n",
        "Unigram accuracy: 0.9194630872483222\n",
        "\n",
        "Bigram accuracy: 0.9463087248322147\n",
        "\n",
        "**With Stop Words and With tf-idf**\n",
        "\n",
        "TF-IDF unigram accuracy 0.8322147651006712\n",
        "\n",
        "TF-IDF bigram accuracy 0.8993288590604027\n",
        "\n",
        "**Without Stop Words**\n",
        "\n",
        "Non-stopword unigram accuracy: 0.959731543624161\n",
        "\n",
        "Non-stopword bigram accuracy: 0.7516778523489933\n",
        "\n",
        "**Without Stop Words and With tf-idf**\n",
        "\n",
        "TF-IDF unigram accuracy 0.9463087248322147\n",
        "\n",
        "TF-IDF bigram accuracy 0.9530201342281879\n",
        "\n",
        "\n",
        "**1) The least accuracy :** \n",
        "\n",
        "***Non-stopword bigram accuracy: 0.7516778523489933***\n",
        "\n",
        "We see that the least accuracy in without stopword bigram, if we try to examine it we see that;\n",
        "\n",
        "\n",
        "entertainment :\n",
        "('won best', 37), ('named best', 36)\n",
        "\n",
        "politics :\n",
        "('told bbc', 86)\n",
        "\n",
        "sport :\n",
        "('told bbc', 50), ('australian open', 48)\n",
        "\n",
        "tech :\n",
        "('told bbc', 54) ('news website', 47)\n",
        "\n",
        "These are unrelated pairs.\n",
        "\n",
        "Another reason is that we are not removing stopwords from the test data. This will be resolved if we remove the stopwords from the test data and send them.\n",
        "\n",
        "**2) The best accuracy :**\n",
        "\n",
        "***Non-stopword unigram accuracy: 0.959731543624161 and non-stopword TF-IDF bigram accuracy 0.9530201342281879***\n",
        "\n",
        "Without stopwords we can reach most related words in unigram so the accuracy is highest. \n",
        "\n",
        "\n",
        "For non-stopword TF-IDF bigram accuracy;\n",
        "\n",
        "**business :**\n",
        "\n",
        "('chief executive'), ('economic growth'), ('mr ebbers'), ('deutsche boerse'), ('oil prices'), ('new york'), ('mr glazer'), ('stock market'), ('fourth quarter'), ('sri lanka')\n",
        "\n",
        "**entertainment :**\n",
        "\n",
        "('box office'), ('new york), ('vera drake'), ('los angeles'), ('million dollar'), ('dollar baby'), ('best actress')\n",
        "\n",
        "**politics :**\n",
        "\n",
        "('mr blair'), ('mr brown'), ('prime minister'), ('mr howard'), ('general election'), ('tony blair'), ('kilroy silk'), ('mr kennedy'), ('lib dems')\n",
        "\n",
        "**sport :**\n",
        "\n",
        "('champions league'), ('davis cup'), ('grand slam'), ('new zealand'), ('world cup'), ('cross country'), ('manchester united)\n",
        "\n",
        "**tech :**\n",
        "\n",
        "('mobile phone'), ('mobile phones'), ('anti virus'), ('high definition'), ('bbc news'), ('ask jeeves'), ('consumer electronics'), ('wi fi')\n",
        "\n",
        "\n",
        "These words are very related and their ratios are usually close to each other when compared to the size of the ratios in the with stop word tf-idf. And they don't need any stopword in between pairs so we don't need to drop the stopwords in test texts.\n",
        "\n",
        "***3) Why doesn't TF-IDF improve accuracy?***\n",
        "\n",
        "We think that it cause of the tf-idf ratio. In this example ;\n",
        "\n",
        "business :\n",
        "\n",
        "[('the', 70.95764876462664), ('to', 32.817908709006126), ('of', 28.381825584034942), ('in', 28.341438544205033), ('and', 21.49897057026608), ('said', 12.296416780507798), ('it', 11.707978624765063), ('that', 11.550008739780894), ('is', 11.539364304631231), ('for', 11.346785316842237)]\n",
        "\n",
        "\"the\" has very large ratio when we compare with \"for\". Then the more related words can't effective in predict. However without stopwords unigram the ratio more close each other;\n",
        "\n",
        "business :\n",
        "\n",
        "[('year', 7.885523855358925), ('mr', 7.524265899440198), ('sales', 6.604490735213493), ('growth', 6.482519702778017), ('economy', 6.30511773311268), ('oil', 5.857026406753185), ('bank', 5.82927728165229), ('market', 5.823054761944759), ('firm', 5.719463036419863), ('new', 5.558714949453762)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gdo0h5qBTV0I"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jg0USrjjLyVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rMT-N02iLyYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvEjB1wE_W-x",
        "outputId": "55e4ace6-1d42-4eb5-ec4d-2fefa6e0c53c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    }
  ]
}